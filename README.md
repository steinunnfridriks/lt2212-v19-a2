# LT2212 V19 Assignment 2

From Asad Sayeed's statistical NLP course at the University of Gothenburg.

My name: STEINUNN RUT FRIÐRIKSDÓTTIR

## Additional instructions
Gendoc generates word vectors from documents that come from two different topics within two subfolders of the reuters-topics folder.
It returns a csv file based on the arguments used while running the program from the terminal. Simdoc takes the file generated by
gendoc and prints cosine similarity for documents from each topics separately as well as from one topic to the other. 

## Results and discussion

### Vocabulary restriction.

I chose to limit my vocabulary to the top 200 most frequent words, because that's
way lower than the 12 thousand word vocabulary and should give an obvious
comparison.

### Result table
                              crude                 grain                   crude2grain           grain2crude
Nr1, full vocabulary          0.3728227501474301    0.32865549299276525     0.3094659275625543    0.31116084178154974
Nr2, top 200                  0.49888781328044135   0.43871725709600995     0.41691894627430287   0.4181276473695625
Nr3, tdidf full vocabulary    0.10684881209178627   0.09889855959023745     0.07329296618536757   0.07408517175733295
Nr4, tdidf top 200            0.32052922814646323   0.2827093570030449      0.23250508953807847   0.23346401740843295
Nr5, svd 100 dimensions       0.5027528152721216    0.4554125860186526      0.4205200644189718    0.42157260004569264
Nr6, svd 1000 dimensions      0.37350413186939413   0.3296379443056033      0.3102247795096723    0.311012499505912
Nr7, tdidf svd 100            0.25816731970644047   0.2237115924259819      0.1768146434308274    0.17854840056888344
Nr8, tdidf svd 1000           0.10790593718001566   0.10001934714565712     0.07396934822787163   0.07477737328126656


### The hypothesis in your own words
I suppose the hypothesis of this experiment must be that when extracting word
counts from this amount of data, you really need to take into account the
frequency of words that have more meaning to the topic. For example, even though
the word "Sushi" might not be the most frequent one overall in a corpus, it
might have a lot of meaning for a subset of the corpus where the topic is
related to Japanese cuisine. By inversing the frequency of the documents,
lower-count, meaningful words can have more value in the computations.  
Then we would for example realize how much three recipes that all share the
ingredient "squid ink" have in common, as opposed to just realizing the irrelevant
fact that the ingredient "egg" is very common in recipes.

### Discussion of trends in results in light of the hypothesis
The result table clearly shows that applying TDIDF to the vectors makes the similarity between different topics and documents
way lower. This would make up for a much more specific type of search (for example when looking for a specific term in a search engine).
The result table also shows that the more data that is available, the lower the similarities become (more precise results).
Applying just SVD to the documents does not appear to lead to more precision, and applying both SVD and TFIDF does not seem to lead to
significantly better results than applying just TDIDF. Like the hypothesis stated, it is important to take into account the frequency
of the most important words of the topic/document, and TDIDF seems to do a pretty good job of doing that.  

## Bonus answers
These results are based purely on word vector counts, taking into account every single word token as an individual feature.
By using ngrams, stemming or lemmatization, we could take into account more context and therefore make the results more precise.
In our version, words such as good and better are two separate word tokens, but with lemmatization, those would be counted as one and the same. Stemming would account for  words such as run and running. Ngrams could provide better context for which word combinations are relevant for which topic or document. It is not the same to talk about running shoes, running the program or running water, and with ngrams, we could see clearly that even though all of these phrases have the word running in common, they are not on the same topic.  
